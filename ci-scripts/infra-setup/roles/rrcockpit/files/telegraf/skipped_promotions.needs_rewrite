# #!/usr/bin/env python
# import argparse
# import json
# import re
# import sys

# import dlrnapi_client
# import influxdb_utils
# import promoter_utils
# import requests
# from diskcache import Cache

# cache = Cache('/tmp/skipped_promotions_cache')
# cache.expire()

# DEFAULT_PROMOTER_BASE_URL = (
#     "https://raw.githubusercontent.com/rdo-infra/ci-config/master"
# )


# promoter_skipping_regex = re.compile(
#     ('.*promoter Skipping promotion of (.*) from (.*) to (.*), '
#      'missing successful jobs: (.*)')
# )
# html_link = "<a href='{}' target='_blank' >{}</a>"

# def get_dlrn_client(base_url, release, distro, component):
#     if component:
#         promoter_config = promoter_utils.get_promoter_config(url,
#                                                              release,
#                                                              distro,
#                                                              component)
#     else:
#         promoter_config = promoter_utils.get_promoter_config(url,
#                                                              release,
#                                                              distro)

#     dlrn_client = promoter_utils.get_dlrn_client(promoter_config)
#     return dlrn_client


# def get_failing_jobs_html(dlrn_client, dlrn_hashes, release_name):
#     failing_jobs_html = ""

#     # If any of the jobs is still in progress
#     in_progress = False

#     dlrn = dlrn_client

#     try:
#         dlrn = 'foo'
#         if dlrn:
#             params = dlrnapi_client.Params2()
#             params.commit_hash = dlrn_hashes['commit_hash']
#             params.distro_hash = dlrn_hashes['distro_hash']
#             params.success = str(False)

#             failing_jobs = dlrn.api_repo_status_get(params)
#             if len(failing_jobs) > 0:
#                 for i, failing_job in enumerate(failing_jobs):
#                     if failing_job.in_progress:
#                         in_progress = True
#                     failing_job_ln = html_link.format(
#                         failing_job.url, failing_job.job_id)

#                     if i > 0:
#                         failing_job_ln += "<br>"
#                     failing_jobs_html += failing_job_ln
#             else:
#                 failing_jobs_html = ("<font color='red'>WARNING</font> "
#                                      "expected perodic jobs have not run")

#     except Exception as e:
#         print(e)
#         pass
#     return (in_progress, failing_jobs_html)


# # FIXME: Use a decorator ?
# def get_cached_failing_jobs_html(dlrn_hashes, release_name):
#     cache_key = "failing_jobs_html_{timestamp}_{repo_hash}".format(
#         **dlrn_hashes)
#     if cache_key not in cache:
#         in_progress, failing_jobs_html = get_failing_jobs_html(
#             dlrn_hashes, release_name)

#         # Only chache if jobs have finished
#         if not in_progress:
#             cache.add(cache_key, failing_jobs_html, expire=259200)

#     return cache[cache_key]


# def parse_skipped_promotions(release_name):
#     skipped_promotions = []
#     promoter_logs = requests.get(
#         "http://38.145.34.55/{}.log".format(release_name))

#     def get_log_time(log_line):
#         log_line_splitted = log_line.split()
#         log_time = "{} {}".format(log_line_splitted[0], log_line_splitted[1])
#         log_time = log_time.split(',')[0]
#         return log_time

#     for log_line in promoter_logs.iter_lines():
#         matched_regex = promoter_skipping_regex.match(log_line)
#         if matched_regex:

#             promotion = json.loads(matched_regex.group(1).replace("'", '"'))
#             repo_hash = promotion['full_hash']
#             failing_jobs = matched_regex.group(3)

#             skipped_promotion = {
#                 'repo_hash': repo_hash,
#                 'from_name': matched_regex.group(2),
#                 'to_name': matched_regex.group(3),
#                 'failing_jobs': failing_jobs,
#                 'timestamp': get_log_time(log_line),
#                 'release': release_name
#             }
#             skipped_promotions.append(skipped_promotion)

#     return skipped_promotions


# def to_influxdb(skipped_promotions):
#     influxdb_lines = []
#     influxdb_format = (
#         "skipped-promotions,repo_hash={repo_hash}"
#         ",release={release},from_name={from_name},"
#         "to_name={to_name} failing_jobs=\"{failing_jobs}\" "
#         "{timestamp}")

#     for skipped_promotion in skipped_promotions:
#         skipped_promotion['timestamp'] = influxdb_utils.format_ts_from_str(
#             skipped_promotion['timestamp'])
#         influxdb_lines.append(influxdb_format.format(**skipped_promotion))

#     return influxdb_lines


# #def main():
# #    release = sys.argv[1]
# #    influxdb_lines = to_influxdb(parse_skipped_promotions(release))
# #    print('\n'.join(influxdb_lines))


# if __name__ == '__main__':

#     parser = argparse.ArgumentParser(
#         description="print out jobs missing from promotion criteria",
#         formatter_class=argparse.ArgumentDefaultsHelpFormatter
#     )

#     parser.add_argument(
#         '--url',
#         default=DEFAULT_PROMOTER_BASE_URL,
#         help='Promotion config settings base URL.'
#     )

#     parser.add_argument(
#         '--release',
#         required=True,
#         choices=[
#             'rhos-17', 'rhos-16.2', 'master', 'victoria', 'ussuri', 'train',
#             'stein', 'rocky', 'queens'
#         ],
#         help='Upstream or downstream release.'
#     )
#     args = parser.parse_args()
#     import pdb
#     pdb.set_trace()
#     get_dlrn_client(args.url, args.release, "CentOS-8", None)

#     main()
